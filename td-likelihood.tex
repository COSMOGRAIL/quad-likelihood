\documentclass[11pt]{scrartcl}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{color}
 
\newcommand{\com}[2]{\xspace\textcolor{red}{\textbf{Comment by #1: #2}}}


\newcommand{\dt}[1]{\ensuremath{\Delta t_{\mathrm{#1}}}\xspace}
%\newcommand{\truedt}{\ensuremath{\Delta t_{\mathrm{true}}}\xspace}
%\newcommand{\modeldt}{\ensuremath{\Delta t_{\mathrm{model}}}\xspace}
%\newcommand{\obsdt}{\ensuremath{\Delta t_{\mathrm{obs}}}\xspace}

\renewcommand{\vec}{\overrightarrow}  % because I like big arrows

\begin{document}

\title{How to express the likelihood of a model given 
COSMOGRAIL time-delay measurements for any lens system}
\subtitle{2nd attempt, VB $\&$ SS, October 2017}
\date{}
\maketitle


This approach is slightly different from the one proposed above. It 
removes the binning in true time delay and "worst random + worst 
systematic" combination used so far in PyCS yet aim to include 2nd 
order 
effects by quantifying how robust the biases, random error and 
covariance coefficients are.\\
\\
A bit of terminology: from our generative model, we draw \emph{mock 
light curves}, or simply \emph{mocks}. We call a collection of 1000 
mocks a \emph{set}. Testing our curve-shifting optimizer on a 
set $n$, without binning the results, provides \emph{biases 
$\vec{b^n}$} and \emph{random errors $\vec{\sigma^n}$} for each 
measured delay, as well as \emph{covariance coefficients $\vec{cov^n}$} 
between each pair of delays.

\section{General formalism}

Let us consider only the measured time delays $\vec{\Delta t_{i}}$ 
relative to a reference image $i$ that we chose a priori - the choice 
should not matter. With respect to this reference image, a set $n$ 
provides then biases $\vec{b^n_i}$, random errors $\vec{\sigma^n_i}$ 
and covariance coefficients $\vec{cov^n_i}$. For example, in a quad 
like HE0435-1223, if we chose image A as reference, a set $n$ 
provides a measurement of $\vec{b^n_A} = [b^n_{AB}, b^n_{AC}, 
b^n_{AD}]$, 
$\vec{\sigma^n_A} = [\sigma^n_{AB}, \sigma^n_{AC}, \sigma^n_{AD}]$ and 
$\vec{cov^n_A} = [cov^n_{ABvAC}, cov^n_{ABvAD}, 
cov^n_{ACvAD}]$\footnote{Note that in the usual covariance formalism, 
$cov^n_{ABvAB} = {\sigma^n_{AB}}^2$}.  It is 
important to note that \emph{these 9 values are independent.} In 
addition, we can compute the average of these vectors over the $N$ 
sets, named $\vec{b^0_i}$, $\vec{\sigma^0_i}$ and $\vec{cov^0_i}$, 
where i.e. $\vec{b^0_i} = \Sigma_{n=1}^N \vec{b^n_i}w_i^n \, / \, 
\Sigma_{n=1}^Nw_i^n$, where for simplicity $w_i^n=1 \,\forall\, i,n$. 
To ease the notation, let us concatenate these vectors into a single 
one $\vec{X^n_i} = 
\vec{b^n_i}+\!\!+\vec{\sigma^n_i}+\!\!+\vec{cov^n_i}$. In the example 
from above using image A of HE0435-1223, that would give us 
$\vec{X^n_A} = [b^n_{AB}, b^n_{AC}, b^n_{AD}, \sigma^n_{AB}, 
\sigma^n_{AC}, \sigma^n_{AD}, cov^n_{ABvAC}, cov^n_{ABvAD}, 
cov^n_{ACvAD}]$. We can similarly compute $\vec{X^0_i}$ by averaging 
over the $\vec{X^n_i}$.\\
\\
With these definitions in hand, we can compute the likelihood 
$P_i(\vec{\Delta t_{i}^{model}}|\vec{\Delta t_{i}}) = P_i$ of modeled 
time delays $\vec{\Delta t_{i}^{model}}$ with respect to measured time 
delays $\vec{\Delta t_{i}}$ with the following equations:


\begin{equation*}
 P_i = N_{\Delta 
t_{i}}\int d\vec{X_i}\,exp\left[-\frac{1}{2}(\vec{\Delta 
t_i^{model}}-\vec{\Delta t_i}-\vec{b_i})^T C(\vec{\Delta 
t_i})^{-1}(\vec{\Delta 
t_i^{model}}-\vec{\Delta t_i}-\vec{b_i})  \right]P(\vec{X_i}),
\end{equation*}


where
\begin{equation*}
 P(\vec{X_i}) = N_Xexp\left[-\frac{1}{2} (\vec{X_i}-\vec{X_i^0})^T 
C(\vec{X_i})^{-1} (\vec{X_i}-\vec{X_i^0})\right],
\end{equation*}

with $N_{\Delta 
t_{i}}$ and $N_X$ being normalization constants that we can safely 
ignore in practice, and $C(\vec{\Delta 
t_i})$ and $C(\vec{X_i})$ being the covariance matrices of 
the elements in $\vec{\Delta 
t_i}$ and $\vec{X_i}$, respectively. The $C(\vec{\Delta 
t_i})$ actually involves the $\vec{\sigma^n_i}$ and $\vec{cov^n_i}$ 
values, hence their presence in $\vec{X_i}$. Note that the $n$ upper 
index has been removed in the equations above for them to make sense 
mathematically, but a numerical resolution of this integral would 
replace the integration over $d\vec{X_i}$ by a sum over $\vec{X_i^n}$.

\section{Simplification and applied notation}

\texttt{GLEE} needs an analytic input for the time-delay likelihood. It 
would be best if we could integrate the equation above, which 
\emph{might} be possible if all the variables are indeed independent 
and have a Gaussian uncertainty distribution. This remains to be 
verified, though. Otherwise, one could still integrate numerically and 
fit the numerical likelihood with a chosen profile.\\
\\
One possible simplification of the equations above is to consider that 
the variance of the random errors $\vec{\sigma^n_i}$ and covariance 
coefficients $\vec{cov^n_i}$ is less important to implement than the 
variance of the bias $\vec{b^n_i}$. In such a case, we could 
approximate $\vec{\sigma^n_i} \simeq \vec{\sigma^0_i}$ and 
$\vec{cov^n_i} \simeq \vec{cov^0_i} \ \forall i$. As a result, the 
measured delays $\vec{\Delta 
t_i^n}$ in the equation above can be considered as constant, measured 
over all sets $\vec{\Delta 
t_i^0}$.\com{VB}{Sherry, do you think we can/should make this 
simplification ?} and the 
covariance matrix of the measured delays $C(\vec{\Delta 
t_i^n})$ becomes a constant $C \simeq C(\vec{\Delta 
t^0_i})$, i.e. the covariance matrix of all the measured delays over 
all sets. There is also no need to integrate over $\vec{\sigma^n_i}$ 
and $\vec{cov^n_i}$, thus $d\vec{X_i}$ becomes simply $d\vec{b_i}$, 
$P(\vec{X_i})$ becomes $P(\vec{b_i})$ and $C(\vec{X_i})$ becomes 
$C(\vec{b_i})$. This simplification has the advantage allowing a direct 
analytic integration of the likelihood expression.   \\
\\
Considering this simplification, let us call $C_{xy}$ and $Cb_{xy}$ 
the coefficients of the $C(\vec{\Delta 
t_i})^{-1}$ and $C(\vec{b_i})^{-1}$ matrices.

\subsection{Quad lens}

For example, HE0435-1223. We pick a reference image, we have thus three 
independent delays. The inverted covariance matrices have 9 
coefficients each, only 6 of which are independent (as $C_{xy}$ = 
$C_{yx}$)


\section{Pros and cons}

\begin{itemize}
 \item \textbf{+} The whole approach is mathematically more consistent 
than above, since we removed the binning in the time-delay measurement 
error
 \item \textbf{+/-} Greatly improves the precision of our measurements 
since we remove the binning of the individual error. TDC results show 
that we are too conservative in our approach when using the free-knot 
splines optimizer (chi2 = 0.5), but roughly reasonable when using 
modified regdiff (chi2 = 1). The precision/accuracy ration needs to be 
tested more intensively using both formalism on TDC data.
 \item ...
\end{itemize}


\section{TODO}
\begin{itemize}
 \item \textbf{VB} - draw ~1000 sets of 1000 mocks each. Ongoing on 
WFI2033, takes a lot of time on a single laptop. Will be 
parallellized and run on clusters.
 \item \textbf{VB} - check the $\vec{b^n_i}$, $\vec{\sigma^n_i}$ and 
$\vec{cov^n_i}$ follow a Gaussian distribution and estimate their 
variance around the $\vec{b^0_i}$, $\vec{\sigma^0_i}$ and 
$\vec{cov^0_i}$.
 \item \textbf{SS} - check if the above equation can be 
analytically integrated.
 
\end{itemize}

If the above works, then we should compare how the final, H0 results 
are affected by considering this approach instead of the older one --> 
test on HE0435-1223 with Ken's models.


\end{document}