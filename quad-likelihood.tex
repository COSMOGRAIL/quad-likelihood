\documentclass[11pt]{scrartcl}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{color}
 
\newcommand{\com}[2]{\xspace\textcolor{red}{\textbf{Comment by #1: #2}}}


\newcommand{\dt}[1]{\ensuremath{\Delta t_{\mathrm{#1}}}\xspace}
%\newcommand{\truedt}{\ensuremath{\Delta t_{\mathrm{true}}}\xspace}
%\newcommand{\modeldt}{\ensuremath{\Delta t_{\mathrm{model}}}\xspace}
%\newcommand{\obsdt}{\ensuremath{\Delta t_{\mathrm{obs}}}\xspace}


\begin{document}

\title{Paragraph on how to express the likelihood of a model given COSMOGRAIL time-delay measurements for quad lenses}
\date{\today}
\maketitle


\section{The current state}

The current way of expressing COSMOGRAIL results for quads is to give 6 dependent -- and consistent -- time-delay estimations, without any estimates of the covariances between these 6 measurements\footnote{We only check visually that the correlations between residuals behave as expected.}. This is described in Section 3 of the PyCS paper. The motivation for giving 6 measurements is that we don't want to pick a priori one of the QSO-images as reference. Note that delivering these 6 \emph{dependent} estimates does contain part of the information that would otherwise go into a covariance matrix associated with giving only 3 ``independent'' delays such as AB, AC and AD. Indeed, a tight covariance between AB and AC (for example) would tell that the delay BC is well constrained. Currently, we just give this delay BC. When fitting a lens model, one currently selects a posteriori 3 ``independent'' delays with small error bars, and one would pick this BC. That's how it was done so far.

\section{The problem}

\begin{itemize}
\item Assuming that these 3 ``independent'' point and uncertainty estimates are truly independent is wrong (the true delays are, but not their measurements). 
\item Selecting the 3 delays among 6 is arbitrary
\item Using all 6 of them without taking into account their covariances is certainly wrong
\end{itemize}

And so it's not fully clear how to write a likelihood of lens model predictions, given COSMOGRAIL measurements.

\section{The fix (under construction)}

We want to propose (and give in our publications) a classical form of the likelihood to be used when modeling our measurements. If we assume that all errors are Gaussian, this would be

\begin{equation}
p(\dt{model} | \mathrm{obs}) = \frac{1}{(2 \pi)^{n/2} |\Sigma|^{1/2}}\exp\left( -\frac{1}{2} (\dt{model}-\dt{obs})^T\Sigma^{-1}(\dt{model}-\dt{obs}) \right)
\end{equation}

where $\Sigma \approx \textrm{Cov}(\dt{obs})$ is an estimation of the covariance that we would give.

\begin{itemize}
\item We \emph{think} that writing this using 3 delays such as (AB, AC, 
AD) is correct, and the associated 3x3 covariance matrix does in fact 
capture the full information contained in our measurements, \emph{even 
if the light curve of A is of poor quality and the choice of BA, BC, BD 
seems apparently ``better''}. The choice of A as a reference does not 
matter, as long as the 3x3 covariance matrix is given together with the 
3 delay estimates. Test this ? \com{VB}{This is indeed completely True, 
when using a covariance matrix of AB, AC and BC with each shifts (A, B, 
C) randomly picked} (don't say "randomly picked", more details would be needed to understand the test and what was tested). \com{VB}{It varies a bit when considering the sub-matrix AB, 
AC, BC created with the PyCS procedure.}


\item For 6 delays, the covariance matrix would be 6x6. But this matrix 
is clearly redundant (BC = AC - AB exactly, for every single 
measurement). A real covariance matrix would be singular in this case, but our PyCS-way of computing it might avoid this. Still, our feeling is that 
using it in the above equation is wrong and would result in an overly 
tight likelihood.\com{VB}{Indeed, we can't use the above equation, 
because it {\textbf won't} work with a matrix whose determinant is 
zero. And it can be proven that determinant=0 if the rows or columns are 
not independent ! It however works in our case because of our tricky way 
to compute the variance using the worst of each bins, but I would tend 
to see this more as a glitch.}


If we want to use the 6x6 matrix nevertheless (because of our special 
conservative PyCS way of computing variances ignoring the true delays), 
can we modify the above equation to compensate for using redundant 
information ? \com{VB}{I would tend to say that we should not use the 
6x6 matrix, since if it was constructed in a robust, mathematical way 
then its full use would be forbidden by the math themselves. We should 
instead provide the four 3x3 sub-matrices and say that we recommend 
either to pick a reference image, or (better!) to run the analysis with 
the four images as reference turn in turn and take the average.}
Yes, I kind of agree.

\end{itemize}
\newpage
\subsection{question for Stackexchange}

Let's say I measure the values of two variables A and B $N$ times
(say $N=1000$), and I build the variable C as a function of A and B, 
e.g. C=A+B.

From my $N$ measurements, I build a vector 
with their mean value $M_{obs}=(A_{obs}, B_{obs}, C_{obs})$ and a 
covariance matrix:

\begin{equation*}
\Sigma=
  \begin{pmatrix}
    cov(AA) & cov(AB) & cov(AC) \\
    cov(AB) & cov(BB) & cov(BC) \\
    cov(AC) & cov(BC) & cov(CC) \\      
  \end{pmatrix}
\end{equation*}


Now, let's say I want to use that covariance matrix to compute a 
probability distribution for a model of the values of A, B and C, that 
I call $M_{model} = (A_{model}, B_{model}, C_{model})$. With 
$n=len(M_{model})$. I have:

\begin{equation*}
p(M_{model} |M_{obs}) = \frac{1}{(2 \pi)^{n/2} 
|\Sigma|^{1/2}}\exp\left( -\frac{1}{2} 
(M_{model}-M_{obs})^T\Sigma^{-1}(M_{model}-M_{obs}) \right)
\end{equation*}


However, since C = A+B, the covariance matrix $\Sigma$ will have a null 
determinant and cannot be inverted. Thus, I cannot compute 
probabilities 
using the equation above.

If I am interested only in the probability distribution of $A_{model}$ I 
can build from my observations the two 
vectors $M_{ABobs}=(A_{obs}, B_{obs})$ and $M_{ACobs} = (A_{obs}, 
C_{obs})$, as well as their associated covariance matrices
\begin{equation*}
\Sigma_{AB}=
  \begin{pmatrix}
    cov(AA) & cov(AB) \\
    cov(AB) & cov(BB) \\     
  \end{pmatrix}
,\ \Sigma_{AC}=
  \begin{pmatrix}
    cov(AA) & cov(AC) \\
    cov(AC) & cov(CC) \\   
  \end{pmatrix}  
\end{equation*}


No surprises, if I plug theses in the equations above, I find the 
same result for the probability distribution of $A_{model}$ when I 
marginalize over $B_{model}$, respectively $C_{model}$, i.e. 
$p(A_{model}|M_{ABobs}) = 
p(A_{model}|M_{ACobs})  \ \forall \ A_{model} $ 

And here come the questions:

 1) Knowing that C=A+B, is it possible to use a modified version of 
$M_{obs}$-$\Sigma$ to get the correct probability distribution without 
using 
$M_{ABobs}$-$\Sigma_{AB}$, or $M_{ACobs}$-$\Sigma_{AC}$? 

 2) Same question, but without knowing the exact function that links 
C to A and B (i.e assuming C can also be directly measured, but due to 
unknown measurement errors it is not exactly A+B)?
 

\newpage
\subsection{Answer for H0LiCOW}

The current way of expressing COSMOGRAIL results for quads is to give 6 
dependent -- and consistent -- time-delay estimations, without any 
estimates of the covariances between these 6 measurements (We 
only check visually that the correlations between residuals behave as 
expected). This is described in Section 3 of the Tewes et al. 2013 
paper. The motivation for giving 6 measurements is that we don't want to 
pick a priori one of the QSO-images as reference, since it might 
introduce a bias in the results as demonstrated by Ken a while ago. 
Note that delivering these 6 \emph{dependent} estimates does contain 
part of the information that would otherwise go into a covariance 
matrix associated with giving only 3 ``independent'' delays such as AB, 
AC and AD. Indeed, a tight covariance between AB and AC (for example) 
would tell that the delay BC is well constrained. Currently, we just 
give this delay BC. When fitting a lens model, one currently selects a 
posteriori 3 ``independent'' delays with small error bars, and one would 
pick this BC. That's how it was done so far.\\

We currently see three issues with that way of doing:

\begin{itemize}
\item Assuming that these 3 ``independent'' point and uncertainty 
estimates are truly independent is wrong (the true delays are, but not 
their measurements). 
\item Selecting the 3 delays among 6 is arbitrary, at least from the 
time-delay measurement point of view.
\item Using all 6 of them without taking into account their covariances 
is certainly wrong.
\end{itemize}

So, how to proceed ?\\

What we want is to propose (and give in our future publications) a 
classical form of the likelihood to be used when modeling our 
measurements. If we assume that all errors are Gaussian, this 
would be

\begin{equation}
p(\dt{model} | \mathrm{obs}) = \frac{1}{(2 \pi)^{n/2} 
|\Sigma|^{1/2}}\exp\left( -\frac{1}{2} 
(\dt{model}-\dt{obs})^T\Sigma^{-1}(\dt{model}-\dt{obs}) \right)
\end{equation}

where $\Sigma \approx \textrm{Cov}(\dt{obs})$ is an estimation of the 
covariance that we would give.


\begin{itemize}
\item We \emph{think} that writing this using 3 \emph{independent} 
delays such as (AB, AC, 
AD) is correct, and the associated 3x3 covariance matrix does in fact 
capture the full information contained in our measurements, \emph{even 
if the light curve of A is of poor quality and the choice of BA, BC, BD 
seems apparently ``better''}. Mock tests showed that the choice of A as 
a reference does not 
matter, as long as the 3x3 covariance matrix is given together with the 
3 delay estimates.


\item For 6 delays, the covariance matrix would be 6x6. But this matrix 
is clearly redundant (BC = AC - AB exactly, for every single 
measurement). A real covariance matrix would be singular (i.e. det=0 
thus the matrix is non-invertible) in this case, but our PyCS-way of 
computing it (being conservative) avoid this. Still, our feeling is 
that 
using it in the above equation is wrong and would always result in an 
overly 
tight likelihood.

To the best of our knowledge, there is no way to modify the equation 
above to properly take into account that redundancy in the 6 delays. We 
will will then instead provide the four 3x3 sub-matrices, i.e. and 
say that we recommend 
either to pick a reference image, or (better!) to run the analysis with 
the four images as reference turn in turn and take the average. 
\com{vb}{or even better, to run the analysis with all the possible 
independent combinations of time delays ?}

\end{itemize}

\subsection*{Computation of the PyCS covariance matrix}

Initially, we wanted the computation of the covariance matrix 
coefficients to follow the same 
principles that we used so far in PyCS (described in Tewes et al. 
2013). When drawing simulated light curves, we give them "true" delays 
picked in a chosen range of possible delays, run the optimizers on each 
simulation, bin the results according to their true delays and then 
measure the systematic + random error in each bins. We then combine the 
worst systematic and worst random error to get the final estimation. 
When constructing the covariance matrix, we keep this procedure for the 
diagonal 
coefficients. For the off-diagonal coefficients, we do a 2d-binning 
(since we compute to covariance of two different delays), using larger 
bins to be 
sure that we have enough estimates per bins, and compute the covariance 
coefficient in each bin. This time however, we cannot simply pick the 
highest or lowest coefficients because 
doing so will not ensure the likelihood to be the most conservative 
one. The two options we have a this stage are:

\begin{enumerate}
\item consider all the possible combination of off-diagonal 
coefficients ($nxn$ 2d-bins would mean $n^6$ combinations for a quad) 
and take the less constraining likelihood; 

\item compute a single likelihood using all the samples (i.e. no 
binning) when computing the off-diagonals coefficients, and make sure 
that when doing the 2d-binning there are no large variations between the 
coefficients computed from one 2d-bin to another. 
\end{enumerate}

The second options seems to be the most natural, so we decided to adopt 
it for the time being. If, for some reason two bins give a very 
discrepant value for an off-diagonal coefficient -in the sense that the 
resulting time-delay likelihood is significantly different, that will 
be interpreted as a sign that something is going on. Whether we should 
worry or not will probably depend on which system we analyze. Note 
that in order to get a scatter in the bins not dominated by random 
fluctuations, we ought to draw more simulated light curves than before. 
10'000 seems to be a reasonable number from the testing we performed so 
far.\\

To give you a visual to chew on, here is the "technical" output plot of 
PyCS on simulated data:\\

\com{vb}{Add Image}\\

The diagonal panels display the time-delay error distributions versus 
the binned true delay estimation. The off-diagonal panels display the 
covariance in each bins. If there are less than a given number of 
simulations per 
bins, the value is discarded (i.e. put to zero, written in red). The 
corresponding delays and mean covariance coefficients on all the 
simulations is 
displayed in the title of each panel. For simplicity, the 6x6 matrix of 
covariances is printed on the right of the plot. The choice of true 
delay range used in the drawing of the simulations as well as the 
number of 1d and 2d bins, minimal 
number of simulations per bins and total number of simulations are 
up to the PyCS user. Obviously, we don't want that these choices 
affect too much the results, and making sure that they don't is part of 
the sanity checks that should be conducted before publishing the final 
time-delay likelihoods.




\end{document}

